# 第一章
==模式识别系统==

![](C:\Users\DONG\Desktop\课件\模式识别\图像 1.png)

**模式** 可以看作是对象的组成成分或影响因素见存在的规律性关系，或者是因素间存在确定性或随机性规律的对象，凡是人类能用其感官直接或间接接受的外界信息都称为模式
**模式识别** 就是对模式的区分和认识
**样本** 所研究对象的一个个体
**样本集** 若干样本的集合
**类或类别** 所有样本上定义的一个子集
**特征** 用于表征样本的观测信息
### 监督模式识别（邻近算法，线性回归，逻辑回归，SVM，决策树……）
已知要划分的类别，并且能够获得一定数量的类别已知的训练样本，在这种情况下建立分类器

已知样本(known sample):事先知道类别标号的样本 （训练样本)
未知样本(unknown sample):类别标号未知但特征已知 的样本（待识别的样本）

特征不是越多越好，测量代价越大，模型越复杂，分类边界也越复杂，
*处理监督模式识别的一般步骤*
1.	分析问题
2.	原始特征获取
3.	特征提取与选择
4.	分类器设计
5.	分类决策

### 半监督模式识别
*	半监督分类
*	半监督聚类

### 非监督模式识别（K均值，C均值，主成分分析，谱聚类算法……）
给定的是未知样本集合，按其特征把相似的归为一类

*处理非监督模式识别的一般步骤*
1.	分析问题
2.	原始特征提取
3.	特征提取与选择
4.	聚类分析
5.	结果解释
****
#第二章 统计学习方法
==贝叶斯决策的两个要求==

*	各个类别的总体概率分布 (先验概率和类条件概率密度) 是已知的
*	要决策分类的类别数是一定的

### 最小错误率贝叶斯决策
1.	已知先验概率、类条件概率密度

2.	计算后验概率

3.	最小错误率决策等价于后验概率最大化决策

### 最小风险贝叶斯决策
1.	利用贝叶斯公式计算后验概率

2.	利用决策表，计算条件风险

3.	决策：在各种决策中选择风险最小的决策

可以看到，同样的数据，因为对两类错误所带来的风险的认识不同，可能得出相反的结论。决策表不同会导致决策结果的不同
****
# 第三章 概率密度函数的估计
### 参数估计（仅从训练样本出发）
概率密度函数形式已知，参数未知或部分知道，利用样本来估计这些参数

*	最大似然估计（参数未知的确定变量）
	1.	给出似然函数（对数似然函数）的形式
	2.	对似然函数（对数似然函数）进行关于参数的求导，并求得导数为0的参数值
	3.	选择对应最大似然函数的参数值作为对参数的最终估计
*	贝叶斯估计（参数未知的随机变量）
	1.	确定参数seita的先验分布密度
	2.	由于样本是独立同分布的，而且已知样本密度函数的形式，可以形式上求出样本集的联合分布
	3.	利用贝叶斯公式求参数seita的后验概率分布
	4.	求参数seita的贝叶斯估计量（公式）

==最大似然估计与贝叶斯估计的比较==

最大似然估计|贝叶斯估计
----------|--------
参数为未知确定变量|参数为为止随机变量
没有利用参数先验信息|利用参数先验信息
估计的概率模型和假设模型一致|估计的概率模型相比假设模型会发生变化
可理解性好|可理解性差
计算简单|计算复杂


### 非参估计（从参数的先验知识和样本出发）
概率密度函数形式未知，利用样本将其数值化估计出来

*	直方图估计
	*	把x的每个分量分成k个等间隔小舱
	*	统计各个落入小舱内的样本数
	*	求相应小舱内的概率密度
	*	窗口位置、大小固定,平均概率密度
*	Parzen窗估计
	*	滑动窗口、窗口大小固定，使用滑动窗口估计每个点的概率密度
*	k近邻估计
	*	固定样本数量Kn ，调整区域体积大小Vn，直至有Kn个样本落入区域中
****
# 第四章 线性分类器
**线性判别函数假定函数g（x）是x的线性函数**

++如何利用样本集求得w和w0++
采用的分类器尽可能地满足设计要求；而设计要求在数学上往往表现为某个特定准则函数

### 线性判别函数的几何意义
>寻找一个最佳投影方向，投影后将原分类问题转化为一维的数据分类问题
总之，利用线性判别函数进行决策，就是用一个超平面把特征空间分割成两个决策区域。超平面的方向权由w确定，位置由阈值w0确定

### Fisher线性判别分析
++要解决的问题++
找到某个方向，使得在这个方向的直线上，样本的投影能分开的最好

*步骤：*

1.	先根据给出的数据，求出类均值向量m~i~
2.	根据给定数据和m~i~，计算类内离散度矩阵S~i~
3.	计算总类内离散度矩阵S~w~=S~1~+S~2~
4.	根据公式求FIsher判别准则下的最优投影方向:w*=Sw(逆)(m~1~-m~2~)
5.	求出投影到一维空间后,两类的均m~i~（波浪）=w（转制）m~i~
6.	b=1/2（m~1~（波浪）+m~2~（波浪））

### 感知器
样本集可分性：在样本的特征空间中，至少存在一个线性分类面能够把两类样本没有错误的分开
如果解存在，则必定在N个超平面的正半空间的相交区（解区）
**感知器准则函数只适合于线性可分的情况**

### 最小平方误差判别
线性不可分情况

### 线性支持向量机
++最优分类超平面++
定义：一个超平面，如果能将训练样本没有错误的分开，并且两类训练样本中离超平面最近的样本与超平面之间的距离是最大的，则这个朝平面叫**最优分类超平面**

++大间隔与推广能力++
要能够在队训练样本正确分类的基础上，对所有可能样本正确分类
过拟合：一个过于复杂的决策界面一般来说不太可能有好的推广能力，它只是针对个别训练样本的调整，而没有真 正地反映所要识别对象模型的本质特征。
推广能力：不仅对训练样本正确分类，要能够对所有可能样本正确分类

++SVM优点++

1.	分类器的复杂度取决于支持向量的个数，而不是变换空间的维数。因此SVM不容易发生过拟合的现象
2.	当原训练样本线性不可分时，可利用某个非线性变换将原数据空间中的样本点映射到更高维空间中，使得在此高维空间中的映射点线性可分
3.	核技巧


**期望风险** 所有可能出现的样本的错误率或风险
**经验风险** 所有训练样本的分类决策损失
### 多类线性分类器
==方法==

1.	把多类问题分解为多个两类问题
2.	直接设计多类分类器

### 设计线性分类器的流程
1.	收集具有类别标识的样本集X
2.	根据实际情况确定一个线性分类器准则函数J
	*	准则函数J是样本集X和未知变量w的函数
	*	准则函数J的值反应分类器的性能
3.	应用最优化技术求解准则函数的极值解w*

### 梯度下降法求解分类器权值的过程
1.	初始化w1，计算出w1处的目标函数梯度
2.	计算w2
3.	计算梯度，第k步迭代
4.	直到迭代收敛或达到给定的停止准则
****
# 第五章 非线性分类器
### 分段线性分类器
*一个非线性函数可以用多段线性函数来逼近
把各类划分为若干子类，使各个子类间的分类用较简单的分类器完成，最后的决策面是由各个子类间的分类面连接而构成得*

问题线性不可分

*	噪声影响
*	问题本身
	*	使用非线性分类器
	*	改变特征，使线性可分
		*	新特征
		*	非线性变换

==三种设计方法==

1.	若已知子类划分，直接用多类线性分类方法
2.	已知子类数目，不知子类划分，用错误修正法
3.	未知子类数目，树状线性分类器


### 基于核的支撑向量机
支持向量机就是采用引入特征变换将原空间中的非线性问题转化为新空间中的线性问题

==SVM基本思想==

1.	通过非线性变换将输入空间变换到一个高维空间
2.	在这个新空间求解最优分类面即最大间隔分类面
3.	非线性变换通过适当的内积核函数实现
4.	求解一个有约束的二次优化问题，有唯一最优解（相比多层感知网络的一个优势）
5.	问题的计算复杂度由样本数目决定，不取决与样本变换特征维数与所采用的核函数

==常用核函数==

*	多项式核函数
*	径向基（RBF）核函数
*	Sigmoid函数

*SVM形式上类似一个神经网络*

==大间隔机器与核函数==

任何线性方法，如果把特征进行适当的转换，就可以得到相应的非线性方法
然而，这种变换会带来两个问题

*	样本维数呈指数增加
*	求解的决策函数复杂度增加，推广能力差

**基于核的支持向量机通过其“大间隔”与“核函数”思想有效解决了这两个问题**

*	核函数：运算不需要在高维的空间里进行。映射后的空间还是无穷维，推广能力还是问题
*	大间隔：控制最大化分类问题，在很高维空间里仍能保持很好的推广能力

==基本思路==

1.	特征x进行非线性变换
2.	优化问题
3.	决策函数

### 核Fisher判别
++基本思想++
采用广义线性判别函数的思想，首先将样本映射到高位空间，然后再新空间里求解Fisher线性判别
### 人工神经网络

==神经网络基本结构==

大量简单的计算单元（结点）以某种形式相连接，形成一个网络，其中的某些因素，如连接强度（权值）、结点计算特性甚至网络结构等，可依某种规则随外部数据进行适当的调整，最终实现某种功能。

*	前馈型神经网络
*	反馈型网络
*	竞争学习网络

# 第六章 其他分类方法

### 近邻法

==最近邻法==
对于一个新样本，把它逐一与已知样本比较，找出距离新样本最近的已知样本，以该样本的类别作为新样本的类别

==k近邻法==
找出x的k个近邻，看其中多数属于哪一类，则把x分到哪一类

1.	计算测试数据与各个训练数据之间的距离
2.	按照距离的递增关系进行排序
3.	选取距离最小的k个点
4.	确定前k个点所在类别的出现频率
5.	返回前k个点中出现频率最高的类别作为测试数据的预测分类

==分支定界算法==

1.	初始化 2.当前节点展开 3.检验 4.回溯 5.选择最近节点 6.检验

==近邻法的快速算法==

1.	把样本集分级分成多个子集
2.	每个子集（节点）可用较少几个量代表
3.	通过将新样本与各节点比较排除大量候选样本
4.	只有最后的节点（子集）中逐个样本比较，找出近邻

### 决策树与随机森林

==非数值特征==

*	名义特征（字符）
*	序数特征（数值，有顺序）
*	与研究目标之间呈非线性的数值特征
*	区间数据

==决策树（可能发生过拟合）==

*决策树是一种树型结构，其中每个内部结点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶结点代表一种类别。
决策树学习采用的是自顶向下的递归方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为零，此时每个叶节点中的实例都属于同一类*

==决策树学习算法的特点==

可以自学习，在学习过程中，不需要使用者了解过多背景知识，只需对训练实例进行较好的标注，就能够进行学习（有监督学习）

**三种决策树学习算法**

*	ID3决策（适应信息增益来进行特征选择的决策树学习过程）
*	C4.5（信息增益率）
*	CART（基尼指数）

### Boosting方法

*	Bagging
	*	基本思路：利用不同样本集合训练单个分类器
*	AdaBoost
	*	基本思路：每一个样本都被赋予一个权重，表明其被选为训练集的概率。如果某样本已被正确分类，则权重降低，否则权重提高。最后根据每个分量分类器的性能，以加权投票的方式进行决策

==分类器并非越多越好==

*	更多的分类器导致更多的计算和存储开销
*	越多的分类器会使个体之间的差异越来越难获得
*	实验证实了过多的分类器可能降低集成的泛化能力

# 第七章 特征选择

### 特征提取
通过映射（或变换）的方法把原始特征变换为较少的新特征（二次特征）

### 特征选择
从原始特征中挑选出一些最具有代表性、最有利于分类的特征作为输入分类器的特征

### 可分性判据
*	基于类内类间距离的可分性判据（定义直观、易于实现、比较常用）
*	基于概率分布的可分性判据
*	基于熵的可分性判据（具有最小不确定性（熵）的特征对于进行分类最为有利）

# 第八章 特征提取

### 主分量分析（PCA，又称K-L变换）
（添加照片内容）


### 多维尺度变换（MDS）
约简后低维空间中任意两点间的距离应该与它们在原高维空间中的距离相同

==算法步骤==

1.	首先计算数据点两两之间的距离矩阵
2.	双中心化距离矩阵，由公式得到内积矩阵B
3.	计算内积矩阵B的特征值和特征向量
4.	把内积矩阵B分解为Y（转至）\*Y，从而得到低维坐标Y

# 第九章 非监督模式识别
### 基于模型的方法
==基本设定==
各类样本的分布是单峰的，根据总体分布中的单峰来划分子集
==投影方法==
基本思想：把样本投影到某一维坐标轴，在这一维上求样本概率密度，根据概率密度的单峰划分子集（投影方向：使方差最大的方向）
==算法步骤==
1.	计算样本协方差矩阵最大本征值对应的本征向量，将样本投影到该方向
2.	估计投影后样本概率密度函数p（v）
3.	求p（v）极小点
4.	若p（v）没有这种极小点，则用下一本征值对应的本征向量作为投影方向
5.	对划分出的每一个子集重复上述过程

==问题==
估计概率密度函数比较困难
需要寻找密度函数中的单峰

### 混合模型估计
### 动态聚类方法
>==相似性度量==
直观理解：同类样本的特征向量应是相互靠近的（前提，特征选取合理，能反映所感兴趣的关系）

动态聚类是多次迭代，逐步调整类别划分，最终使某准则达到最优

==要点==

*	选某种距离作为相似性样本度量
*	定义某个准则函数，用于评价聚类质量
*	出事分类方法及迭代算法

### K均值聚类算法
样本类别数已知

==k均值算法基本思想==
通过迭代寻找k个聚类的一种划分方案，使得用这k个聚类代表相应各类样本，所得到的总体误差最小

==k均值算法基本步骤==

1.	为待聚类的点寻找聚类中心（初始化）
2.	计算每个点道具类中心的距离，将每个点聚类到离该点最近的聚类中去
3.	计算每个聚类中所有点的坐标平均值，并将这个平均值作为新的聚类中心
4.	反复执行2，3，直到聚类中心不再进行大范围移动或者聚类次数达到要求为止

==k均值算法主要流程==
### C均值聚类算法

==C均值算法基本思想==
通过迭代寻找c个聚类的一种划分方案，使得用这c个聚类的均值代表相应各类样本时，所得到的总体误差最小
### ISODATA方法
==特点==

* 	成批样本修正，把所有样本调整完后才重新计算均值
* 	可进行类别合并与分裂

==算法步骤==
1.初始化 2.把所有样本分到距离最近的类中 3.若某类中样本数过少，则去掉这一类，置c=c-1 4.重新计算均值 5.计算第j类样本与其中心的平均距离和总平均距离 6.是最后一次迭代，则程序停止 7.分裂 8.合并 9.重复6